{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\a931e\\AppData\\Local\\Temp\\ipykernel_18808\\63983185.py\", line 33, in start\n",
      "  File \"c:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\asyncio\\runners.py\", line 44, in run\n",
      "    return loop.run_until_complete(main)\n",
      "  File \"c:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\asyncio\\base_events.py\", line 642, in run_until_complete\n",
      "    return future.result()\n",
      "  File \"C:\\Users\\a931e\\AppData\\Local\\Temp\\ipykernel_18808\\63983185.py\", line 37, in main\n",
      "  File \"c:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\site-packages\\websockets\\legacy\\server.py\", line 1068, in __aenter__\n",
      "    return await self\n",
      "  File \"c:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\site-packages\\websockets\\legacy\\server.py\", line 1086, in __await_impl__\n",
      "    server = await self._create_server()\n",
      "  File \"c:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\asyncio\\base_events.py\", line 1494, in create_server\n",
      "    raise OSError(err.errno, 'error while attempting '\n",
      "OSError: [Errno 10048] error while attempting to bind on address ('::1', 8765, 0, 0): only one usage of each socket address (protocol/network address/port) is normally permitted\n",
      "connection handler failed\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\site-packages\\websockets\\legacy\\protocol.py\", line 944, in transfer_data\n",
      "    message = await self.read_message()\n",
      "  File \"c:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\site-packages\\websockets\\legacy\\protocol.py\", line 1013, in read_message\n",
      "    frame = await self.read_data_frame(max_size=self.max_size)\n",
      "  File \"c:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\site-packages\\websockets\\legacy\\protocol.py\", line 1089, in read_data_frame\n",
      "    frame = await self.read_frame(max_size)\n",
      "  File \"c:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\site-packages\\websockets\\legacy\\protocol.py\", line 1144, in read_frame\n",
      "    frame = await Frame.read(\n",
      "  File \"c:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\site-packages\\websockets\\legacy\\framing.py\", line 70, in read\n",
      "    data = await reader(2)\n",
      "  File \"c:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\asyncio\\streams.py\", line 721, in readexactly\n",
      "    raise exceptions.IncompleteReadError(incomplete, n)\n",
      "asyncio.exceptions.IncompleteReadError: 0 bytes read on a total of 2 expected bytes\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\site-packages\\websockets\\legacy\\server.py\", line 231, in handler\n",
      "    await self.ws_handler(self)\n",
      "  File \"C:\\Users\\a931e\\AppData\\Local\\Temp\\ipykernel_18808\\63983185.py\", line 45, in echo\n",
      "    async for message in websocket:\n",
      "  File \"c:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\site-packages\\websockets\\legacy\\protocol.py\", line 481, in __aiter__\n",
      "    yield await self.recv()\n",
      "  File \"c:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\site-packages\\websockets\\legacy\\protocol.py\", line 552, in recv\n",
      "    await self.ensure_open()\n",
      "  File \"c:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\site-packages\\websockets\\legacy\\protocol.py\", line 920, in ensure_open\n",
      "    raise self.connection_closed_exc()\n",
      "websockets.exceptions.ConnectionClosedError: no close frame received or sent\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import websockets, asyncio\n",
    "import threading\n",
    "\n",
    "class WaitableQueue(asyncio.Queue):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.event = threading.Event()\n",
    "\n",
    "    def put(self, item):\n",
    "        super().put_nowait(item)\n",
    "        self.event.set()\n",
    "\n",
    "    def get(self,timeout=3):\n",
    "        if self.event.wait(timeout):\n",
    "            res = super().get_nowait()\n",
    "            if super().empty():\n",
    "                self.event.clear()\n",
    "            return res\n",
    "        else:\n",
    "            raise TimeoutError(\"Environement is not responding.\")\n",
    "\n",
    "# the server\n",
    "class Server:\n",
    "    def __init__(self):\n",
    "        self.inQueue = WaitableQueue()\n",
    "        self.outQueue = WaitableQueue()\n",
    "        self.debug = True\n",
    "        self.ws = None\n",
    "\n",
    "    def start(self):\n",
    "        threading.Thread(target=self.message_sender_loop).start()\n",
    "        asyncio.run(self.main())\n",
    "\n",
    "    async def main(self):\n",
    "        try:\n",
    "            async with websockets.serve(self.echo, \"localhost\", 8765):\n",
    "                await asyncio.Future()  # run forever\n",
    "        except websockets.exceptions.ConnectionClosedError as e: print(e)\n",
    "\n",
    "    async def echo(self,websocket):\n",
    "        self.ws = websocket\n",
    "        print('connect')\n",
    "        #asyncio.create_task(self.message_sender_loop())\n",
    "        async for message in websocket:\n",
    "            try:\n",
    "                self.recv(json.loads(message))\n",
    "            except json.decoder.JSONDecodeError:\n",
    "                self.recv(message)\n",
    "\n",
    "    def recv(self,message):\n",
    "        self.inQueue.put(message)\n",
    "        if self.debug:\n",
    "            print(\"recv: \",message)\n",
    "    \n",
    "    def send(self,command:str, content):\n",
    "        self.outQueue.put({'command':command,'content':content})\n",
    "\n",
    "    def message_sender_loop(self):\n",
    "        while True:\n",
    "            try:\n",
    "                message = self.outQueue.get(None)\n",
    "                asyncio.run(self.ws.send(json.dumps(message, indent=4)))\n",
    "            except websockets.exceptions.ConnectionClosedError:\n",
    "                print(\"Connection closed\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "\n",
    "# start the server in a separate thread to avoid blocking\n",
    "import threading\n",
    "server = Server()\n",
    "t=threading.Thread(target=server.start)\n",
    "t.start()\n",
    "\n",
    "# the interface to the server\n",
    "class WSManager:\n",
    "    def __init__(self,server:Server):\n",
    "        self.debug = False\n",
    "        self.server = server\n",
    "\n",
    "#server.send(\"action\",{\"voltage\":[1,0,0,0,100,200,100,100]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "def flatten(list_of_lists):\n",
    "    if len(list_of_lists) == 0:\n",
    "        return list(list_of_lists)\n",
    "    if hasattr(list_of_lists[0], '__iter__'):\n",
    "        return flatten(list_of_lists[0]) + flatten(list_of_lists[1:])\n",
    "    return list(list_of_lists[:1]) + flatten(list_of_lists[1:])\n",
    "def decomposeCosSin(angle):\n",
    "    return [np.cos(angle), np.sin(angle)]\n",
    "def processFeature(state:dict,targetPos):\n",
    "    feature = []\n",
    "    feature.append(state['baseLinkPos']['x']-targetPos[0].item())\n",
    "    feature.append(state['baseLinkPos']['y']-targetPos[1].item())\n",
    "    feature.append(decomposeCosSin(state['baseLinkOrientation']))\n",
    "    feature.append(state['baseLinkVelocity']['x'])\n",
    "    feature.append(state['baseLinkVelocity']['y'])\n",
    "    feature.append(state['baseLinkAngularVelocity'])\n",
    "    feature.append(decomposeCosSin(state['wheelBaseOrientation']))\n",
    "    feature.append(state['wheelSpeed'])\n",
    "    feature = flatten(feature)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Q(nn.Module):\n",
    "    def __init__(self,state_size,action_size,hidden_size):\n",
    "        super(Q, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.fc = nn.Sequential(\n",
    "        nn.Linear(state_size+action_size,hidden_size),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(hidden_size,hidden_size),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(hidden_size,1)\n",
    "        )\n",
    "\n",
    "    def forward(self,state,action):\n",
    "        return self.fc(torch.cat([state,action],dim=1))\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self,state_size,action_size,hidden_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.fc = nn.Sequential(\n",
    "        nn.Linear(state_size,hidden_size),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(hidden_size,hidden_size),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(hidden_size,action_size)\n",
    "        )\n",
    "\n",
    "    def forward(self,state):\n",
    "        return self.fc(state)\n",
    "\n",
    "import random, ou\n",
    "class Environment:\n",
    "    def __init__(self,ws_server : Server,device = 'cpu'):\n",
    "        self.ws = ws_server\n",
    "        self.replayBuffer = []\n",
    "        self.t = 0\n",
    "        self.t_episode = 0\n",
    "        self.device = device\n",
    "        self.prevState = None\n",
    "        self.prevAction = None\n",
    "        self.pos = None\n",
    "        self.targetPos = None\n",
    "        self.ouNoise = ou.ND_OUNoise(8,0, 0.1, 0.2, 0, 100)\n",
    "        self.noiseIntensity = 0.5\n",
    "        self.targetRelPos = torch.tensor([0.,-3.])\n",
    "\n",
    "    def restartEpisode(self):\n",
    "        self.targetPos = self.pos + self.targetRelPos\n",
    "        self.t_episode = 0\n",
    "        self.prevState = None\n",
    "        self.ws.send(\"new target\",{\"pos\":{'x':self.targetPos[0].item(),'y':0, 'z':self.targetPos[1].item()}})\n",
    "\n",
    "    def calculateReward(self,pos,targetPos):\n",
    "        return -torch.dist(pos,targetPos)\n",
    "\n",
    "    def terminateCondition(self,pos,targetPos):\n",
    "        return torch.dist(pos,targetPos)<0.5 or torch.dist(pos,targetPos)>10\n",
    "\n",
    "    def getPos(self,state):\n",
    "        return torch.tensor([state['baseLinkPos']['x'],state['baseLinkPos']['y']],dtype=torch.float32)\n",
    "\n",
    "    def update(self, policy: torch.nn.Module):\n",
    "        raw_state = None\n",
    "        while not server.inQueue.empty():\n",
    "            message = server.inQueue.get()\n",
    "            if message['command'] == 'state':\n",
    "                raw_state = message['content']\n",
    "        if raw_state:\n",
    "            # If the environment returns a state, the step is finnished.\n",
    "            self.pos = self.getPos(raw_state)\n",
    "            if self.t_episode > 100 or self.t == 0 or self.terminateCondition(self.pos,self.targetPos):\n",
    "                self.restartEpisode()\n",
    "            state = torch.tensor(processFeature(raw_state,self.targetPos),dtype=torch.float32).to(self.device)\n",
    "                \n",
    "            # Add the experience to the replay buffer.\n",
    "            if self.t_episode > 0: # Skip the first step.\n",
    "                reward = self.calculateReward(self.pos,self.targetPos)- self.calculateReward(self.prevPos,self.targetPos)#-(torch.max(torch.zeros_like(self.prevAction),(torch.abs(self.prevAction)-2000))**2).mean()*0.001\n",
    "                self.replayBuffer.append((state,self.prevAction,reward,self.prevState))\n",
    "                if len(self.replayBuffer) > 5000:\n",
    "                    self.replayBuffer.pop(random.randint(0,len(self.replayBuffer)-1))\n",
    "            \n",
    "            # Give the new action to enable the environment to continue on the next step.\n",
    "            with torch.no_grad():\n",
    "                policy.eval()\n",
    "                action = policy(state).detach().cpu()\n",
    "                action += self.ouNoise.__next__()*self.noiseIntensity\n",
    "                #action[5]=action[6]=action[7]=action[4]\n",
    "                action = torch.clamp(action,-4000,4000)\n",
    "            self.ws.send(\"action\",{\"voltage\":list(action.detach().numpy().tolist())})\n",
    "\n",
    "            \n",
    "            self.t+=1\n",
    "            self.t_episode += 1\n",
    "            self.prevState = state\n",
    "            self.prevAction = action\n",
    "            self.prevPos = self.getPos(raw_state)\n",
    "\n",
    "    def sampleExperience(self,batch_size):\n",
    "        ns,a,r,s = zip(*random.sample(self.replayBuffer,batch_size))\n",
    "        return torch.stack(ns),torch.stack(a),torch.stack(r),torch.stack(s)\n",
    "#env = Environment(server,device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "env = Environment(server,device)\n",
    "tau = 0.001\n",
    "gamma = 0.9\n",
    "batch_size = 128\n",
    "q = Q(state_size=19,action_size=8,hidden_size=512)\n",
    "q_target = Q(state_size=19,action_size=8,hidden_size=512)\n",
    "policy = Policy(state_size=19,action_size=8,hidden_size=512)\n",
    "policy_target = Policy(state_size=19,action_size=8,hidden_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Q(state_size=19,action_size=8,hidden_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimQ = torch.optim.Adam(q.parameters(),lr=0.001)\n",
    "optimPolicy = torch.optim.Adam(policy.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection closed\n",
      "q loss: 0.0011696324218064547, policy loss: -0.5803424119949341\n",
      "connect\n",
      "q loss: 0.002539626555517316, policy loss: -0.4132908582687378\n",
      "q loss: 0.0036600064486265182, policy loss: -5.776938438415527\n",
      "q loss: 0.0032082637771964073, policy loss: -6.259049892425537\n",
      "q loss: 0.00295313261449337, policy loss: -6.366705417633057\n",
      "q loss: 0.0024401224218308926, policy loss: -8.23759651184082\n",
      "q loss: 0.0021614395081996918, policy loss: -7.441329002380371\n",
      "q loss: 0.00507876044139266, policy loss: -8.41982650756836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "connection handler failed\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\site-packages\\websockets\\legacy\\protocol.py\", line 944, in transfer_data\n",
      "    message = await self.read_message()\n",
      "  File \"c:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\site-packages\\websockets\\legacy\\protocol.py\", line 1013, in read_message\n",
      "    frame = await self.read_data_frame(max_size=self.max_size)\n",
      "  File \"c:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\site-packages\\websockets\\legacy\\protocol.py\", line 1089, in read_data_frame\n",
      "    frame = await self.read_frame(max_size)\n",
      "  File \"c:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\site-packages\\websockets\\legacy\\protocol.py\", line 1144, in read_frame\n",
      "    frame = await Frame.read(\n",
      "  File \"c:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\site-packages\\websockets\\legacy\\framing.py\", line 70, in read\n",
      "    data = await reader(2)\n",
      "  File \"c:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\asyncio\\streams.py\", line 721, in readexactly\n",
      "    raise exceptions.IncompleteReadError(incomplete, n)\n",
      "asyncio.exceptions.IncompleteReadError: 0 bytes read on a total of 2 expected bytes\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\site-packages\\websockets\\legacy\\server.py\", line 231, in handler\n",
      "    await self.ws_handler(self)\n",
      "  File \"C:\\Users\\a931e\\AppData\\Local\\Temp\\ipykernel_18808\\63983185.py\", line 45, in echo\n",
      "    async for message in websocket:\n",
      "  File \"c:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\site-packages\\websockets\\legacy\\protocol.py\", line 481, in __aiter__\n",
      "    yield await self.recv()\n",
      "  File \"c:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\site-packages\\websockets\\legacy\\protocol.py\", line 552, in recv\n",
      "    await self.ensure_open()\n",
      "  File \"c:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\site-packages\\websockets\\legacy\\protocol.py\", line 920, in ensure_open\n",
      "    raise self.connection_closed_exc()\n",
      "websockets.exceptions.ConnectionClosedError: no close frame received or sent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connect\n",
      "q loss: 0.0018345353892073035, policy loss: -7.121649265289307\n",
      "q loss: 0.005829949863255024, policy loss: -1.8156616687774658\n",
      "q loss: 0.0025570630095899105, policy loss: -6.7298970222473145\n",
      "q loss: 0.00744173489511013, policy loss: -5.754608631134033\n",
      "q loss: 0.0029427488334476948, policy loss: -5.638504505157471\n",
      "q loss: 0.0016132415039464831, policy loss: -6.957573890686035\n",
      "q loss: 0.0015990696847438812, policy loss: -6.50506591796875\n",
      "q loss: 0.0071762800216674805, policy loss: -0.7182192802429199\n",
      "q loss: 0.0026863086968660355, policy loss: -2.2355542182922363\n",
      "q loss: 0.00300349248573184, policy loss: -2.094534397125244\n",
      "q loss: 0.0021662903018295765, policy loss: -2.0946590900421143\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mw:\\robotics\\RobotSimulation\\RobotSimulationAI\\Untitled.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/robotics/RobotSimulation/RobotSimulationAI/Untitled.ipynb#ch0000005?line=60'>61</a>\u001b[0m     optimPolicy\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/robotics/RobotSimulation/RobotSimulationAI/Untitled.ipynb#ch0000005?line=61'>62</a>\u001b[0m     policy_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/w%3A/robotics/RobotSimulation/RobotSimulationAI/Untitled.ipynb#ch0000005?line=62'>63</a>\u001b[0m     optimPolicy\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/robotics/RobotSimulation/RobotSimulationAI/Untitled.ipynb#ch0000005?line=64'>65</a>\u001b[0m \u001b[39m# Update the target networks.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/robotics/RobotSimulation/RobotSimulationAI/Untitled.ipynb#ch0000005?line=65'>66</a>\u001b[0m soft_update_target(q_target,q,tau)\n",
      "File \u001b[1;32mc:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\site-packages\\torch\\optim\\optimizer.py:87\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/a931e/Anaconda3/envs/nn/lib/site-packages/torch/optim/optimizer.py?line=84'>85</a>\u001b[0m obj, \u001b[39m*\u001b[39m_ \u001b[39m=\u001b[39m args\n\u001b[0;32m     <a href='file:///c%3A/Users/a931e/Anaconda3/envs/nn/lib/site-packages/torch/optim/optimizer.py?line=85'>86</a>\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m---> <a href='file:///c%3A/Users/a931e/Anaconda3/envs/nn/lib/site-packages/torch/optim/optimizer.py?line=86'>87</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m     <a href='file:///c%3A/Users/a931e/Anaconda3/envs/nn/lib/site-packages/torch/optim/optimizer.py?line=87'>88</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\site-packages\\torch\\autograd\\profiler.py:435\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/a931e/Anaconda3/envs/nn/lib/site-packages/torch/autograd/profiler.py?line=433'>434</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__enter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> <a href='file:///c%3A/Users/a931e/Anaconda3/envs/nn/lib/site-packages/torch/autograd/profiler.py?line=434'>435</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49m_record_function_enter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m    <a href='file:///c%3A/Users/a931e/Anaconda3/envs/nn/lib/site-packages/torch/autograd/profiler.py?line=435'>436</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "def soft_update_target(target:nn.Module, source:nn.Module,tau):\n",
    "    for t, s in zip(target.parameters(), source.parameters()):\n",
    "        t.data.copy_(\n",
    "            (1. - tau) * t.data + tau * s.data)\n",
    "\n",
    "server.debug = False\n",
    "q.train()\n",
    "policy.train()\n",
    "q_target.eval()\n",
    "policy_target.eval()\n",
    "\n",
    "q.to(device)\n",
    "policy.to(device)\n",
    "q_target.to(device)\n",
    "policy_target.to(device)\n",
    "\n",
    "policy_loss = torch.tensor(torch.nan)\n",
    "\n",
    "import time\n",
    "# Fill the replay buffer with random experiences.\n",
    "while len(env.replayBuffer) < batch_size+1:\n",
    "    env.update(policy)\n",
    "    time.sleep(0.02)\n",
    "\n",
    "# Training.\n",
    "for t in range(100000000):\n",
    "    env.update(policy)\n",
    "\n",
    "    new_state, action, reward, old_state = env.sampleExperience(batch_size)\n",
    "    new_state = new_state.to(device)\n",
    "    old_state = old_state.to(device)\n",
    "    action = action.to(device)\n",
    "    reward = reward.to(device)\n",
    "\n",
    "    q_target.eval()\n",
    "    policy_target.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        action_ = policy_target(new_state)\n",
    "        torch.clamp(action_,-4000,4000)\n",
    "        new_value = q_target(new_state,action_).detach()\n",
    "        target_value = reward.unsqueeze(1) #+ gamma*new_value\n",
    "    \n",
    "    #target_value = reward.unsqueeze(1)\n",
    "    q.train()\n",
    "    policy.train()\n",
    "\n",
    "    # Update the Q network.\n",
    "    q_loss = F.mse_loss(q(old_state,action),target_value)\n",
    "    optimQ.zero_grad()\n",
    "    q_loss.backward()\n",
    "    optimQ.step()\n",
    "    \n",
    "    if q_loss.item()<50:\n",
    "        # Update the policy network.\n",
    "        q.eval()\n",
    "        action = policy(old_state)\n",
    "        voltage_penalty = (torch.max(torch.zeros_like(action),(torch.abs(action)-2000))**2).mean()*1\n",
    "        policy_loss = -q(old_state,action).mean() + voltage_penalty\n",
    "        optimPolicy.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimPolicy.step()\n",
    "    \n",
    "    # Update the target networks.\n",
    "    soft_update_target(q_target,q,tau)\n",
    "    #soft_update_target(policy_target,policy,tau)\n",
    "    \n",
    "    if t % 1000 == 0:\n",
    "        print(f\"q loss: {q_loss.item()}, policy loss: {policy_loss.item()}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.noiseIntensity = 0.1\n",
    "env.targetRelPos = torch.tensor([0,3],dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([969.1476], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor([2144.1477], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor(-0.0388, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "new_state, action, reward, old_state = env.sampleExperience(batch_size)\n",
    "new_state = new_state.to(device)\n",
    "old_state = old_state.to(device)\n",
    "action = action.to(device)\n",
    "reward = reward.to(device)\n",
    "print(q(old_state,action)[0])\n",
    "action = policy(old_state)\n",
    "print(q(old_state,action)[0])\n",
    "print(reward[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object WebSocketCommonProtocol.send at 0x0000014D8CB5C640>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server.ws.send({\"voltage\":list(action.detach().cpu().numpy().tolist())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2141, -0.2820, -0.2855, -0.3078, -0.1516, -0.0655, -0.3429, -0.2886,\n",
       "        -0.3183, -0.1440, -0.1253, -0.3222, -0.2555, -0.3395, -0.1388, -0.1747,\n",
       "        -0.2437, -0.2255,  0.0972, -0.3279, -0.3285, -0.2891, -0.1524, -0.3033,\n",
       "        -0.3226, -0.1426, -0.2665, -0.1397, -0.2500, -0.3043, -0.3331, -0.1957,\n",
       "        -0.0817, -0.1915, -0.2449,  0.0132, -0.2816, -0.2662, -0.2573, -0.2298,\n",
       "        -0.1589, -0.2701, -0.3098, -0.0501, -0.0601, -0.3006, -0.2713, -0.0870,\n",
       "        -0.2514, -0.3083, -0.1783, -0.2688, -0.2306, -0.2980, -0.1285,  0.0586,\n",
       "        -0.3285, -0.2772, -0.1705, -0.1688, -0.2943, -0.1653, -0.2915, -0.3086,\n",
       "        -0.2993, -0.1145, -0.0723, -0.3109, -0.1668, -0.3018, -0.1963, -0.2858,\n",
       "        -0.3372, -0.2997, -0.3229, -0.3214, -0.2191, -0.3148, -0.3162, -0.2828,\n",
       "         0.1556,  0.0275, -0.2177, -0.3105, -0.2920, -0.2034, -0.2636, -0.1015,\n",
       "        -0.3027,  0.1306, -0.2835, -0.3080,  0.0337, -0.1594, -0.2613, -0.2097,\n",
       "        -0.3227, -0.1678, -0.2023, -0.3349, -0.0421, -0.1362, -0.3238, -0.2226,\n",
       "         0.1466, -0.3252, -0.2976, -0.3154,  0.0051, -0.1678, -0.2595,  0.0978,\n",
       "         0.0829, -0.2185, -0.1681, -0.1085, -0.3139,  0.0382, -0.3079, -0.1236,\n",
       "        -0.2814, -0.0355,  0.1052, -0.2424, -0.0461, -0.2920, -0.2405, -0.1065])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state, action, reward, old_state = env.sampleExperience(batch_size)\n",
    "reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WaitableQueue at 0x131ce69a580 maxsize=0 _queue=[{'command': 'action', 'content': {'voltage': [0, 0, 0, 0, 0, 0, 0, 0]}}, {'command': 'action', 'content': {'voltage': [0, 0, 0, 0, 0, 0, 0, 0]}}, {'command': 'action', 'content': {'voltage': [0, 0, 0, 0, 0, 0, 0, 0]}}, {'command': 'action', 'content': {'voltage': [0, 0, 0, 0, 0, 0, 0, 0]}}, {'command': 'action', 'content': {'voltage': [0, 0, 0, 0, 0, 0, 0, 0]}}, {'command': 'action', 'content': {'voltage': [0, 0, 0, 0, 0, 0, 0, 0]}}, {'command': 'action', 'content': {'voltage': [0, 0, 0, 0, 0, 0, 0, 0]}}] tasks=8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server.outQueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c9624776d510bbc019f6ff189fc1336bb72bf6a7e32ae91ed9f1b177f47b3d26"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
