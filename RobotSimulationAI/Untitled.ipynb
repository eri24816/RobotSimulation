{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import websockets, asyncio\n",
    "import threading\n",
    "\n",
    "class WaitableQueue(asyncio.Queue):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.event = threading.Event()\n",
    "\n",
    "    def put(self, item):\n",
    "        super().put_nowait(item)\n",
    "        self.event.set()\n",
    "\n",
    "    def get(self,timeout=3):\n",
    "        if self.event.wait(timeout):\n",
    "            res = super().get_nowait()\n",
    "            if super().empty():\n",
    "                self.event.clear()\n",
    "            return res\n",
    "        else:\n",
    "            raise TimeoutError(\"Environement is not responding.\")\n",
    "\n",
    "# the server\n",
    "class Server:\n",
    "    def __init__(self):\n",
    "        self.inQueue = WaitableQueue()\n",
    "        self.outQueue = WaitableQueue()\n",
    "        self.debug = True\n",
    "        self.ws = None\n",
    "\n",
    "    def start(self):\n",
    "        threading.Thread(target=self.message_sender_loop).start()\n",
    "        asyncio.run(self.main())\n",
    "\n",
    "    async def main(self):\n",
    "        try:\n",
    "            async with websockets.serve(self.echo, \"localhost\", 8765):\n",
    "                await asyncio.Future()  # run forever\n",
    "        except websockets.exceptions.ConnectionClosedError as e: print(e)\n",
    "\n",
    "    async def echo(self,websocket):\n",
    "        self.ws = websocket\n",
    "        print('connect')\n",
    "        #asyncio.create_task(self.message_sender_loop())\n",
    "        async for message in websocket:\n",
    "            try:\n",
    "                self.recv(json.loads(message))\n",
    "            except json.decoder.JSONDecodeError:\n",
    "                self.recv(message)\n",
    "\n",
    "    def recv(self,message):\n",
    "        self.inQueue.put(message)\n",
    "        if self.debug:\n",
    "            print(\"recv: \",message)\n",
    "    \n",
    "    def send(self,command:str, content):\n",
    "        self.outQueue.put({'command':command,'content':content})\n",
    "\n",
    "    def message_sender_loop(self):\n",
    "        while True:\n",
    "            try:\n",
    "                message = self.outQueue.get(None)\n",
    "                asyncio.run(self.ws.send(json.dumps(message, indent=4)))\n",
    "            except websockets.exceptions.ConnectionClosedError:\n",
    "                print(\"Connection closed\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "\n",
    "# start the server in a separate thread to avoid blocking\n",
    "import threading\n",
    "server = Server()\n",
    "t=threading.Thread(target=server.start)\n",
    "t.start()\n",
    "\n",
    "# the interface to the server\n",
    "class WSManager:\n",
    "    def __init__(self,server:Server):\n",
    "        self.debug = False\n",
    "        self.server = server\n",
    "\n",
    "#server.send(\"action\",{\"voltage\":[1,0,0,0,100,200,100,100]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "def flatten(list_of_lists):\n",
    "    if len(list_of_lists) == 0:\n",
    "        return list(list_of_lists)\n",
    "    if hasattr(list_of_lists[0], '__iter__'):\n",
    "        return flatten(list_of_lists[0]) + flatten(list_of_lists[1:])\n",
    "    return list(list_of_lists[:1]) + flatten(list_of_lists[1:])\n",
    "def decomposeCosSin(angle):\n",
    "    return [np.cos(angle), np.sin(angle)]\n",
    "def processFeature(state:dict,targetPos):\n",
    "    feature = []\n",
    "    feature.append(state['baseLinkPos']['x']-targetPos[0].item())\n",
    "    feature.append(state['baseLinkPos']['y']-targetPos[1].item())\n",
    "    feature.append(decomposeCosSin(state['baseLinkOrientation']))\n",
    "    feature.append(state['baseLinkVelocity']['x'])\n",
    "    feature.append(state['baseLinkVelocity']['y'])\n",
    "    feature.append(state['baseLinkAngularVelocity'])\n",
    "    feature.append(decomposeCosSin(state['wheelBaseOrientation']))\n",
    "    feature.append(state['wheelSpeed'])\n",
    "    feature = flatten(feature)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Q(nn.Module):\n",
    "    def __init__(self,state_size,action_size,hidden_size):\n",
    "        super(Q, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.fc = nn.Sequential(\n",
    "        nn.Linear(state_size+action_size,hidden_size),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(hidden_size,hidden_size),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(hidden_size,1)\n",
    "        )\n",
    "\n",
    "    def forward(self,state,action):\n",
    "        return self.fc(torch.cat([state,action],dim=1))\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self,state_size,action_size,hidden_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.fc = nn.Sequential(\n",
    "        nn.Linear(state_size,hidden_size),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(hidden_size,hidden_size),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(hidden_size,action_size)\n",
    "        )\n",
    "\n",
    "    def forward(self,state):\n",
    "        return self.fc(state)\n",
    "\n",
    "import random, ou\n",
    "class Environment:\n",
    "    def __init__(self,ws_server : Server,device = 'cpu'):\n",
    "        self.ws = ws_server\n",
    "        self.replayBuffer = []\n",
    "        self.t = 0\n",
    "        self.t_episode = 0\n",
    "        self.device = device\n",
    "        self.prevState = None\n",
    "        self.prevAction = None\n",
    "        self.pos = None\n",
    "        self.targetPos = None\n",
    "        self.ouNoise = ou.ND_OUNoise(8,0, 0.1, 0.2, 0, 100)\n",
    "        self.noiseIntensity = 0.5\n",
    "\n",
    "    def restartEpisode(self):\n",
    "        self.targetPos = self.pos + torch.tensor([0.,-3.])\n",
    "        self.t_episode = 0\n",
    "        self.prevState = None\n",
    "        self.ws.send(\"new target\",{\"pos\":{'x':self.targetPos[0].item(),'y':0, 'z':self.targetPos[1].item()}})\n",
    "\n",
    "    def calculateReward(self,pos,targetPos):\n",
    "        return -torch.dist(pos,targetPos)\n",
    "\n",
    "    def terminateCondition(self,pos,targetPos):\n",
    "        return torch.dist(pos,targetPos)<0.5 or torch.dist(pos,targetPos)>10\n",
    "\n",
    "    def getPos(self,state):\n",
    "        return torch.tensor([state['baseLinkPos']['x'],state['baseLinkPos']['y']],dtype=torch.float32)\n",
    "\n",
    "    def update(self, policy: torch.nn.Module):\n",
    "        raw_state = None\n",
    "        while not server.inQueue.empty():\n",
    "            message = server.inQueue.get()\n",
    "            if message['command'] == 'state':\n",
    "                raw_state = message['content']\n",
    "        if raw_state:\n",
    "            # If the environment returns a state, the step is finnished.\n",
    "            self.pos = self.getPos(raw_state)\n",
    "            if self.t_episode > 100 or self.t == 0 or self.terminateCondition(self.pos,self.targetPos):\n",
    "                self.restartEpisode()\n",
    "            state = torch.tensor(processFeature(raw_state,self.targetPos),dtype=torch.float32).to(self.device)\n",
    "                \n",
    "            # Add the experience to the replay buffer.\n",
    "            if self.t_episode > 0: # Skip the first step.\n",
    "                reward = self.calculateReward(self.pos,self.targetPos)- self.calculateReward(self.prevPos,self.targetPos)#-(torch.max(torch.zeros_like(self.prevAction),(torch.abs(self.prevAction)-2000))**2).mean()*0.001\n",
    "                self.replayBuffer.append((state,self.prevAction,reward,self.prevState))\n",
    "                if len(self.replayBuffer) > 5000:\n",
    "                    self.replayBuffer.pop(random.randint(0,len(self.replayBuffer)-1))\n",
    "            \n",
    "            # Give the new action to enable the environment to continue on the next step.\n",
    "            with torch.no_grad():\n",
    "                policy.eval()\n",
    "                action = policy(state).detach().cpu()\n",
    "                action += self.ouNoise.__next__()*self.noiseIntensity\n",
    "                #action[5]=action[6]=action[7]=action[4]\n",
    "                action = torch.clamp(action,-4000,4000)\n",
    "            self.ws.send(\"action\",{\"voltage\":list(action.detach().numpy().tolist())})\n",
    "\n",
    "            \n",
    "            self.t+=1\n",
    "            self.t_episode += 1\n",
    "            self.prevState = state\n",
    "            self.prevAction = action\n",
    "            self.prevPos = self.getPos(raw_state)\n",
    "\n",
    "    def sampleExperience(self,batch_size):\n",
    "        ns,a,r,s = zip(*random.sample(self.replayBuffer,batch_size))\n",
    "        return torch.stack(ns),torch.stack(a),torch.stack(r),torch.stack(s)\n",
    "#env = Environment(server,device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "env = Environment(server,device)\n",
    "tau = 0.001\n",
    "gamma = 0.9\n",
    "batch_size = 128\n",
    "q = Q(state_size=19,action_size=8,hidden_size=512)\n",
    "q_target = Q(state_size=19,action_size=8,hidden_size=512)\n",
    "policy = Policy(state_size=19,action_size=8,hidden_size=512)\n",
    "policy_target = Policy(state_size=19,action_size=8,hidden_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Q(state_size=19,action_size=8,hidden_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimQ = torch.optim.Adam(q.parameters(),lr=0.001)\n",
    "optimPolicy = torch.optim.Adam(policy.parameters(),lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q loss: 747.9697265625, policy loss: nan\n",
      "q loss: 0.05969659984111786, policy loss: -0.5737780332565308\n",
      "q loss: 0.015236292965710163, policy loss: -1.0960254669189453\n",
      "q loss: 0.009553283452987671, policy loss: 0.2910975217819214\n",
      "q loss: 0.005607171915471554, policy loss: 0.266168475151062\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "def soft_update_target(target:nn.Module, source:nn.Module,tau):\n",
    "    for t, s in zip(target.parameters(), source.parameters()):\n",
    "        t.data.copy_(\n",
    "            (1. - tau) * t.data + tau * s.data)\n",
    "\n",
    "server.debug = False\n",
    "q.train()\n",
    "policy.train()\n",
    "q_target.eval()\n",
    "policy_target.eval()\n",
    "\n",
    "q.to(device)\n",
    "policy.to(device)\n",
    "q_target.to(device)\n",
    "policy_target.to(device)\n",
    "\n",
    "policy_loss = torch.tensor(torch.nan)\n",
    "\n",
    "import time\n",
    "# Fill the replay buffer with random experiences.\n",
    "while len(env.replayBuffer) < batch_size+1:\n",
    "    env.update(policy)\n",
    "    time.sleep(0.02)\n",
    "\n",
    "# Training.\n",
    "for t in range(100000000):\n",
    "    env.update(policy)\n",
    "\n",
    "    new_state, action, reward, old_state = env.sampleExperience(batch_size)\n",
    "    new_state = new_state.to(device)\n",
    "    old_state = old_state.to(device)\n",
    "    action = action.to(device)\n",
    "    reward = reward.to(device)\n",
    "\n",
    "    q_target.eval()\n",
    "    policy_target.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        action_ = policy_target(new_state)\n",
    "        torch.clamp(action_,-4000,4000)\n",
    "        new_value = q_target(new_state,action_).detach()\n",
    "        target_value = reward.unsqueeze(1) #+ gamma*new_value\n",
    "    \n",
    "    #target_value = reward.unsqueeze(1)\n",
    "    q.train()\n",
    "    policy.train()\n",
    "\n",
    "    # Update the Q network.\n",
    "    q_loss = F.mse_loss(q(old_state,action),target_value)\n",
    "    optimQ.zero_grad()\n",
    "    q_loss.backward()\n",
    "    optimQ.step()\n",
    "    \n",
    "    if q_loss.item()<50:\n",
    "        # Update the policy network.\n",
    "        q.eval()\n",
    "        action = policy(old_state)\n",
    "        voltage_penalty = (torch.max(torch.zeros_like(action),(torch.abs(action)-2000))**2).mean()*1\n",
    "        policy_loss = -q(old_state,action).mean() + voltage_penalty\n",
    "        optimPolicy.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimPolicy.step()\n",
    "    \n",
    "    # Update the target networks.\n",
    "    soft_update_target(q_target,q,tau)\n",
    "    #soft_update_target(policy_target,policy,tau)\n",
    "    \n",
    "    if t % 1000 == 0:\n",
    "        print(f\"q loss: {q_loss.item()}, policy loss: {policy_loss.item()}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3171586.5000, device='cuda:0', grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([969.1476], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor([2144.1477], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor(-0.0388, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "new_state, action, reward, old_state = env.sampleExperience(batch_size)\n",
    "new_state = new_state.to(device)\n",
    "old_state = old_state.to(device)\n",
    "action = action.to(device)\n",
    "reward = reward.to(device)\n",
    "print(q(old_state,action)[0])\n",
    "action = policy(old_state)\n",
    "print(q(old_state,action)[0])\n",
    "print(reward[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.3109e-06, device='cuda:0', grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2141, -0.2820, -0.2855, -0.3078, -0.1516, -0.0655, -0.3429, -0.2886,\n",
       "        -0.3183, -0.1440, -0.1253, -0.3222, -0.2555, -0.3395, -0.1388, -0.1747,\n",
       "        -0.2437, -0.2255,  0.0972, -0.3279, -0.3285, -0.2891, -0.1524, -0.3033,\n",
       "        -0.3226, -0.1426, -0.2665, -0.1397, -0.2500, -0.3043, -0.3331, -0.1957,\n",
       "        -0.0817, -0.1915, -0.2449,  0.0132, -0.2816, -0.2662, -0.2573, -0.2298,\n",
       "        -0.1589, -0.2701, -0.3098, -0.0501, -0.0601, -0.3006, -0.2713, -0.0870,\n",
       "        -0.2514, -0.3083, -0.1783, -0.2688, -0.2306, -0.2980, -0.1285,  0.0586,\n",
       "        -0.3285, -0.2772, -0.1705, -0.1688, -0.2943, -0.1653, -0.2915, -0.3086,\n",
       "        -0.2993, -0.1145, -0.0723, -0.3109, -0.1668, -0.3018, -0.1963, -0.2858,\n",
       "        -0.3372, -0.2997, -0.3229, -0.3214, -0.2191, -0.3148, -0.3162, -0.2828,\n",
       "         0.1556,  0.0275, -0.2177, -0.3105, -0.2920, -0.2034, -0.2636, -0.1015,\n",
       "        -0.3027,  0.1306, -0.2835, -0.3080,  0.0337, -0.1594, -0.2613, -0.2097,\n",
       "        -0.3227, -0.1678, -0.2023, -0.3349, -0.0421, -0.1362, -0.3238, -0.2226,\n",
       "         0.1466, -0.3252, -0.2976, -0.3154,  0.0051, -0.1678, -0.2595,  0.0978,\n",
       "         0.0829, -0.2185, -0.1681, -0.1085, -0.3139,  0.0382, -0.3079, -0.1236,\n",
       "        -0.2814, -0.0355,  0.1052, -0.2424, -0.0461, -0.2920, -0.2405, -0.1065])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state, action, reward, old_state = env.sampleExperience(batch_size)\n",
    "reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WaitableQueue at 0x131ce69a580 maxsize=0 _queue=[{'command': 'action', 'content': {'voltage': [0, 0, 0, 0, 0, 0, 0, 0]}}, {'command': 'action', 'content': {'voltage': [0, 0, 0, 0, 0, 0, 0, 0]}}, {'command': 'action', 'content': {'voltage': [0, 0, 0, 0, 0, 0, 0, 0]}}, {'command': 'action', 'content': {'voltage': [0, 0, 0, 0, 0, 0, 0, 0]}}, {'command': 'action', 'content': {'voltage': [0, 0, 0, 0, 0, 0, 0, 0]}}, {'command': 'action', 'content': {'voltage': [0, 0, 0, 0, 0, 0, 0, 0]}}, {'command': 'action', 'content': {'voltage': [0, 0, 0, 0, 0, 0, 0, 0]}}] tasks=8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server.outQueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c9624776d510bbc019f6ff189fc1336bb72bf6a7e32ae91ed9f1b177f47b3d26"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
