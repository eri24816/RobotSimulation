{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import websockets, asyncio\n",
    "import threading\n",
    "\n",
    "class WaitableQueue(asyncio.Queue):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.event = threading.Event()\n",
    "\n",
    "    def put(self, item):\n",
    "        super().put_nowait(item)\n",
    "        self.event.set()\n",
    "\n",
    "    def get(self,timeout=3):\n",
    "        if self.event.wait(timeout):\n",
    "            res = super().get_nowait()\n",
    "            if super().empty():\n",
    "                self.event.clear()\n",
    "            return res\n",
    "        else:\n",
    "            raise TimeoutError(\"Environement is not responding.\")\n",
    "\n",
    "# the server\n",
    "class Server:\n",
    "    def __init__(self):\n",
    "        self.inQueue = WaitableQueue()\n",
    "        self.outQueue = WaitableQueue()\n",
    "        self.debug = True\n",
    "        self.ws = None\n",
    "\n",
    "    def start(self):\n",
    "        threading.Thread(target=self.message_sender_loop).start()\n",
    "        asyncio.run(self.main())\n",
    "\n",
    "    async def main(self):\n",
    "        try:\n",
    "            async with websockets.serve(self.echo, \"localhost\", 8765):\n",
    "                await asyncio.Future()  # run forever\n",
    "        except websockets.exceptions.ConnectionClosedError as e: print(e)\n",
    "\n",
    "    async def echo(self,websocket):\n",
    "        self.ws = websocket\n",
    "        print('connect')\n",
    "        #asyncio.create_task(self.message_sender_loop())\n",
    "        async for message in websocket:\n",
    "            try:\n",
    "                self.recv(json.loads(message))\n",
    "            except json.decoder.JSONDecodeError:\n",
    "                self.recv(message)\n",
    "\n",
    "    def recv(self,message):\n",
    "        self.inQueue.put(message)\n",
    "        if self.debug:\n",
    "            print(\"recv: \",message)\n",
    "    \n",
    "    def send(self,command:str, content):\n",
    "        self.outQueue.put({'command':command,'content':content})\n",
    "\n",
    "    def message_sender_loop(self):\n",
    "        while True:\n",
    "            try:\n",
    "                message = self.outQueue.get(None)\n",
    "                asyncio.run(self.ws.send(json.dumps(message, indent=4)))\n",
    "            except websockets.exceptions.ConnectionClosedError:\n",
    "                print(\"Connection closed\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "                \n",
    "# start the server in a separate thread to avoid blocking\n",
    "import threading\n",
    "server = Server()\n",
    "t=threading.Thread(target=server.start)\n",
    "t.start()\n",
    "\n",
    "# the interface to the server\n",
    "class WSManager:\n",
    "    def __init__(self,server:Server):\n",
    "        self.debug = False\n",
    "        self.server = server\n",
    "\n",
    "#server.send(\"action\",{\"voltage\":[1,0,0,0,100,200,100,100]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from utils import Stat\n",
    "def flatten(list_of_lists):\n",
    "    if len(list_of_lists) == 0:\n",
    "        return list(list_of_lists)\n",
    "    if hasattr(list_of_lists[0], '__iter__'):\n",
    "        return flatten(list_of_lists[0]) + flatten(list_of_lists[1:])\n",
    "    return list(list_of_lists[:1]) + flatten(list_of_lists[1:])\n",
    "def decomposeCosSin(angle):\n",
    "    return [np.cos(angle), np.sin(angle)]\n",
    "def processFeature(state:dict,targetPos):\n",
    "    feature = []\n",
    "    feature.append(state['baseLinkPos']['x']-targetPos[0].item())\n",
    "    feature.append(state['baseLinkPos']['y']-targetPos[1].item())\n",
    "    feature.append(decomposeCosSin(state['baseLinkOrientation']))\n",
    "    feature.append(state['baseLinkVelocity']['x'])\n",
    "    feature.append(state['baseLinkVelocity']['y'])\n",
    "    feature.append(state['baseLinkAngularVelocity'])\n",
    "    feature.append(decomposeCosSin(state['wheelBaseOrientation']))\n",
    "    feature.append(state['wheelSpeed'])\n",
    "    feature = flatten(feature)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Q(nn.Module):\n",
    "    def __init__(self,state_size,action_size,hidden_size):\n",
    "        super(Q, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.fc = nn.Sequential(\n",
    "        nn.Linear(state_size+action_size,hidden_size),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.LayerNorm(hidden_size),\n",
    "        nn.Linear(hidden_size,hidden_size),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.LayerNorm(hidden_size),\n",
    "        nn.Linear(hidden_size,1)\n",
    "        )\n",
    "\n",
    "    def forward(self,state,action):\n",
    "        return self.fc(torch.cat([state,action],dim=1))\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self,state_size,action_size,hidden_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.fc = nn.Sequential(\n",
    "        nn.Linear(state_size,hidden_size),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.LayerNorm(hidden_size),\n",
    "        nn.Linear(hidden_size,hidden_size),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.LayerNorm(hidden_size),\n",
    "        nn.Linear(hidden_size,action_size)\n",
    "        )\n",
    "\n",
    "    def forward(self,state):\n",
    "        return self.fc(state)\n",
    "\n",
    "import random, ou\n",
    "class Environment:\n",
    "    def __init__(self,ws_server : Server,device = 'cpu'):\n",
    "        self.ws = ws_server\n",
    "        self.replayBuffer = []\n",
    "        self.t = 0\n",
    "        self.t_episode = 0\n",
    "        self.device = device\n",
    "        self.prevState = None\n",
    "        self.prevAction = None\n",
    "        self.pos = None\n",
    "        self.targetPos = None\n",
    "        self.ouNoise = ou.ND_OUNoise(8,0, 0.1, 1, 0, 100)\n",
    "        self.noiseIntensity = 0.5\n",
    "        self.targetRelPos = torch.tensor([0.,-3.])\n",
    "\n",
    "    def restartEpisode(self):\n",
    "        self.pos = torch.tensor([0.,0.])\n",
    "        self.targetPos = self.pos + self.targetRelPos\n",
    "        self.t_episode = 0\n",
    "        self.prevState = None\n",
    "        self.ws.send(\"new target\",{\"pos\":{'x':self.targetPos[0].item(),'y':0, 'z':self.targetPos[1].item()}})\n",
    "        self.ws.send(\"pos\",{'x':0,'y':0, 'z':0})\n",
    "\n",
    "    def calculateReward(self,pos,targetPos):\n",
    "        return -torch.dist(pos,targetPos)\n",
    "\n",
    "    def terminateCondition(self,pos,targetPos):\n",
    "        return torch.dist(pos,targetPos)<0.5 or torch.dist(pos,targetPos)>10\n",
    "\n",
    "    def getPos(self,state):\n",
    "        return torch.tensor([state['baseLinkPos']['x'],state['baseLinkPos']['y']],dtype=torch.float32)\n",
    "\n",
    "    def update(self, policy: torch.nn.Module):\n",
    "        raw_state = None\n",
    "        reward = None\n",
    "        while not server.inQueue.empty():\n",
    "            message = server.inQueue.get()\n",
    "            if message['command'] == 'state':\n",
    "                raw_state = message['content']\n",
    "        if raw_state:\n",
    "            # If the environment returns a state, the step is finnished.\n",
    "            self.pos = self.getPos(raw_state)\n",
    "            if self.t_episode > 100 or self.t == 0 or self.terminateCondition(self.pos,self.targetPos):\n",
    "                self.restartEpisode()\n",
    "                self.ws.send(\"require state\",None)\n",
    "                self.t+=1\n",
    "                return\n",
    "            state = torch.tensor(processFeature(raw_state,self.targetPos),dtype=torch.float32).to(self.device)\n",
    "                \n",
    "            # Add the experience to the replay buffer.\n",
    "            if self.t_episode > 0: # Skip the first step.\n",
    "                reward = self.calculateReward(self.pos,self.targetPos) - self.calculateReward(self.prevPos,self.targetPos)#-(torch.max(torch.zeros_like(self.prevAction),(torch.abs(self.prevAction)-2000))**2).mean()*0.001\n",
    "                self.replayBuffer.append((state,self.prevAction,reward,self.prevState))\n",
    "                if len(self.replayBuffer) > 5000:\n",
    "                    self.replayBuffer.pop(random.randint(0,len(self.replayBuffer)-1))\n",
    "            \n",
    "            # Give the new action to enable the environment to continue on the next step.\n",
    "            with torch.no_grad():\n",
    "                policy.eval()\n",
    "                action = policy(state).detach().cpu()\n",
    "                action += self.ouNoise.__next__()*self.noiseIntensity\n",
    "                #action[5]=action[6]=action[7]=action[4]\n",
    "                action = torch.clamp(action,-2000,2000)\n",
    "            self.ws.send(\"action\",{\"voltage\":list(action.detach().numpy().tolist())})\n",
    "\n",
    "            \n",
    "            self.t+=1\n",
    "            self.t_episode += 1\n",
    "            self.prevState = state\n",
    "            self.prevAction = action\n",
    "            self.prevPos = self.getPos(raw_state)\n",
    "\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    def sampleExperience(self,batch_size):\n",
    "        ns,a,r,s = zip(*random.sample(self.replayBuffer,batch_size))\n",
    "        return torch.stack(ns),torch.stack(a),torch.stack(r),torch.stack(s)\n",
    "\n",
    "#env = Environment(server,device)\n",
    "from torch.nn import functional as F\n",
    "def soft_update_target(target:nn.Module, source:nn.Module,tau):\n",
    "    for t, s in zip(target.parameters(), source.parameters()):\n",
    "        t.data.copy_(\n",
    "            (1. - tau) * t.data + tau * s.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = Stat(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "env = Environment(server,device)\n",
    "tau = 1-0.5**(1/1000)\n",
    "gamma = 0.5**(1/50)\n",
    "batch_size = 128\n",
    "q = Q(state_size=19,action_size=8,hidden_size=512)\n",
    "q_target = Q(state_size=19,action_size=8,hidden_size=512)\n",
    "policy = Policy(state_size=19,action_size=8,hidden_size=512)\n",
    "policy_target = Policy(state_size=19,action_size=8,hidden_size=512)\n",
    "soft_update_target(q_target,q,1)\n",
    "soft_update_target(policy_target,policy,1)\n",
    "t=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimQ = torch.optim.Adam(q.parameters(),lr=0.001)\n",
    "optimPolicy = torch.optim.Adam(policy.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.targetRelPos = torch.tensor([-2,-2],dtype=torch.float32)\n",
    "env.noiseIntensity = 0.1\n",
    "gamma = 0.5**(1/50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_loss: 2.139124976697524e-07\tpolicy_loss: -0.004005931591063534\treward: 0.010850011133680157\t\n",
      "q_loss: 1.8017916886128077e-07\tpolicy_loss: -0.0028716662680256604\treward: 0.011299118099596677\t\n",
      "reward: 0.015366461920359778\tq_loss: 2.4164795769829835e-07\tpolicy_loss: -0.00240625922792761\t\n",
      "policy_loss: -0.0020541238367439408\treward: 0.018572075885929143\tq_loss: 2.6383175346735965e-07\t\n",
      "q_loss: 1.8595675932940262e-07\tpolicy_loss: -0.002012301348206646\treward: 0.015370791894767937\t\n",
      "policy_loss: -0.0017454575761355346\tq_loss: 2.7710877032493434e-07\treward: 0.014183410656489549\t\n",
      "q_loss: 2.5554162123026837e-07\tpolicy_loss: -0.001501945312309208\treward: 0.010024834375876885\t\n",
      "q_loss: 2.2861099241595848e-07\tpolicy_loss: -0.0013599882557237084\treward: 0.012124028386948984\t\n",
      "q_loss: 2.3665547967417527e-07\tpolicy_loss: -0.001206102111402224\treward: 0.01196230858091324\t\n",
      "policy_loss: -0.0012229730094591522\treward: 0.020116439947845243\tq_loss: 3.3194262296958456e-07\t\n",
      "policy_loss: -0.0013874995504522289\treward: 0.1006005724993619\tq_loss: 2.828087196078619e-07\t\n",
      "q_loss: 3.5604266194032745e-07\tpolicy_loss: -0.0015808146131678885\treward: 0.08227244040495912\t\n",
      "reward: 0.05459267291529425\tq_loss: 4.966028132863633e-07\tpolicy_loss: -0.0014025771376368604\t\n",
      "q_loss: 2.7486404873809176e-05\tpolicy_loss: -0.00166118877428484\treward: 0.03300460106553042\t\n",
      "policy_loss: -0.0025197337266789485\treward: 0.03894812126096301\tq_loss: 8.118521650883714e-07\t\n",
      "reward: 0.030124761733501853\tq_loss: 6.802702889834628e-07\tpolicy_loss: -0.002128910317761493\t\n",
      "policy_loss: -0.0021297805242439997\treward: -0.029478696691633458\tq_loss: 7.537699127689514e-07\t\n",
      "q_loss: 1.2612544385084675e-06\tpolicy_loss: -0.0020288376852595604\treward: 0.027644055433013813\t\n",
      "reward: -0.013467829735552678\tq_loss: 1.322391587068288e-06\tpolicy_loss: -0.0020892999993143363\t\n",
      "policy_loss: -0.001770577978767802\treward: -0.03318975334761465\tq_loss: 1.620578359586969e-06\t\n",
      "q_loss: 1.7103112528665017e-06\tpolicy_loss: -0.001982118536522778\treward: 0.023175461093584697\t\n",
      "q_loss: 1.5899240551835249e-06\tpolicy_loss: -0.002011169069713136\treward: 0.05040360656049517\t\n",
      "reward: 0.039001059492561115\tq_loss: 1.3498885685489119e-06\tpolicy_loss: -0.0016062913611082608\t\n",
      "policy_loss: -0.0013165100656184572\treward: -0.03788212288732398\tq_loss: 1.2103574461079428e-06\t\n",
      "policy_loss: -0.0012159682929580283\treward: -0.03209890822569529\tq_loss: 1.3238752058392882e-06\t\n",
      "policy_loss: -0.0010092099477420602\treward: 0.07821320752574973\tq_loss: 1.2648512841601177e-06\t\n",
      "policy_loss: -0.0012483701481566338\treward: 0.02939876988265014\tq_loss: 1.4159731438101639e-06\t\n",
      "q_loss: 2.4920524783992054e-06\tpolicy_loss: -0.0012927060486017969\treward: 0.00926805071292385\t\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mw:\\robotics\\RobotSimulation\\RobotSimulationAI\\Untitled.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/robotics/RobotSimulation/RobotSimulationAI/Untitled.ipynb#ch0000009?line=57'>58</a>\u001b[0m     policy_loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mq(old_state,action)\u001b[39m.\u001b[39mmean() \u001b[39m+\u001b[39m voltage_penalty\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/robotics/RobotSimulation/RobotSimulationAI/Untitled.ipynb#ch0000009?line=58'>59</a>\u001b[0m     optimPolicy\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/w%3A/robotics/RobotSimulation/RobotSimulationAI/Untitled.ipynb#ch0000009?line=59'>60</a>\u001b[0m     policy_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/robotics/RobotSimulation/RobotSimulationAI/Untitled.ipynb#ch0000009?line=60'>61</a>\u001b[0m     optimPolicy\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/robotics/RobotSimulation/RobotSimulationAI/Untitled.ipynb#ch0000009?line=62'>63</a>\u001b[0m \u001b[39m# Update the target networks.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\site-packages\\torch\\_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/a931e/Anaconda3/envs/nn/lib/site-packages/torch/_tensor.py?line=297'>298</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/a931e/Anaconda3/envs/nn/lib/site-packages/torch/_tensor.py?line=298'>299</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/a931e/Anaconda3/envs/nn/lib/site-packages/torch/_tensor.py?line=299'>300</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/a931e/Anaconda3/envs/nn/lib/site-packages/torch/_tensor.py?line=300'>301</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/a931e/Anaconda3/envs/nn/lib/site-packages/torch/_tensor.py?line=304'>305</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/a931e/Anaconda3/envs/nn/lib/site-packages/torch/_tensor.py?line=305'>306</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/a931e/Anaconda3/envs/nn/lib/site-packages/torch/_tensor.py?line=306'>307</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\a931e\\Anaconda3\\envs\\nn\\lib\\site-packages\\torch\\autograd\\__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/a931e/Anaconda3/envs/nn/lib/site-packages/torch/autograd/__init__.py?line=150'>151</a>\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/a931e/Anaconda3/envs/nn/lib/site-packages/torch/autograd/__init__.py?line=151'>152</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m--> <a href='file:///c%3A/Users/a931e/Anaconda3/envs/nn/lib/site-packages/torch/autograd/__init__.py?line=153'>154</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[0;32m    <a href='file:///c%3A/Users/a931e/Anaconda3/envs/nn/lib/site-packages/torch/autograd/__init__.py?line=154'>155</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/a931e/Anaconda3/envs/nn/lib/site-packages/torch/autograd/__init__.py?line=155'>156</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "server.debug = False\n",
    "q.train()\n",
    "policy.train()\n",
    "q_target.eval()\n",
    "policy_target.eval()\n",
    "\n",
    "q.to(device)\n",
    "policy.to(device)\n",
    "q_target.to(device)\n",
    "policy_target.to(device)\n",
    "\n",
    "policy_loss = torch.tensor(torch.nan)\n",
    "\n",
    "import time\n",
    "# Fill the replay buffer with random experiences.\n",
    "while len(env.replayBuffer) < batch_size+1:\n",
    "    reward = env.update(policy)\n",
    "\n",
    "# Training.\n",
    "while(True):\n",
    "    reward = env.update(policy)\n",
    "    if reward is not None:\n",
    "        stat.add('reward',reward.mean().item())\n",
    "        #print(env.t_episode, reward)\n",
    "\n",
    "    new_state, action, reward, old_state = env.sampleExperience(batch_size)\n",
    "    new_state = new_state.to(device)\n",
    "    old_state = old_state.to(device)\n",
    "    action = action.to(device)\n",
    "    reward = reward.to(device)\n",
    "\n",
    "    q_target.eval()\n",
    "    policy_target.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        action_ = policy_target(new_state)\n",
    "        action_ = torch.clamp(action_,-2000,2000)\n",
    "        new_value = q_target(new_state,action_).detach()\n",
    "        \n",
    "        gamma_normalizer = 1/(1-gamma) \n",
    "        target_value = (reward.unsqueeze(1) + gamma*new_value)/gamma_normalizer\n",
    "    \n",
    "    #target_value = reward.unsqueeze(1)\n",
    "    q.train()\n",
    "    policy.train()\n",
    "\n",
    "    # Update the Q network.\n",
    "    q_loss = F.mse_loss(q(old_state,action),target_value)\n",
    "    optimQ.zero_grad()\n",
    "    q_loss.backward()\n",
    "    optimQ.step()\n",
    "    \n",
    "    if q_loss.item()<50:\n",
    "        # Update the policy network.\n",
    "        q.eval()\n",
    "        action = policy(old_state)\n",
    "        voltage_penalty = (torch.max(torch.zeros_like(action),(torch.abs(action)-2000))**2).mean()*1\n",
    "        policy_loss = -q(old_state,action).mean() + voltage_penalty\n",
    "        optimPolicy.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimPolicy.step()\n",
    "    \n",
    "    # Update the target networks.\n",
    "    soft_update_target(q_target,q,tau)\n",
    "    soft_update_target(policy_target,policy,tau)\n",
    "    \n",
    "    stat.set_epoch(t)\n",
    "    stat.add('q_loss',q_loss.item())\n",
    "    stat.add('policy_loss',policy_loss.item())\n",
    "\n",
    "    t+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    action_ = policy_target(new_state)\n",
    "    action_ = torch.clamp(action_,-4000,4000)\n",
    "    new_value = q_target(new_state,action_).detach()\n",
    "    target_value = reward.unsqueeze(1) + gamma*new_value\n",
    "\n",
    "#target_value = reward.unsqueeze(1)\n",
    "q.train()\n",
    "policy.train()\n",
    "\n",
    "# Update the Q network.\n",
    "q_loss = F.mse_loss(q(old_state,action),target_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.208390712738037,\n",
       " -6.874842166900635,\n",
       " 0.9999638199806213,\n",
       " 0.008509729988873005,\n",
       " 0.011094008572399616,\n",
       " -1.4876433610916138,\n",
       " 5.153154779691249e-05,\n",
       " 0.9999638199806213,\n",
       " 0.9999638199806213,\n",
       " 0.9999638199806213,\n",
       " 0.9999638199806213,\n",
       " 0.008509775623679161,\n",
       " 0.008509775623679161,\n",
       " 0.008509775623679161,\n",
       " 0.008509775623679161,\n",
       " -0.5228272676467896,\n",
       " 0.4100436270236969,\n",
       " 0.17000515758991241,\n",
       " -0.2825700640678406]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state[11].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0002e+00, 2.0080e+00, 1.0000e+00, 2.4688e-04, 1.3071e-02, 5.0415e-01,\n",
       "        1.6878e-04, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 2.4657e-04,\n",
       "        2.4657e-04, 2.4657e-04, 2.4657e-04, 8.4878e-02, 1.7258e-01, 9.0376e-02,\n",
       "        1.2012e-01], device='cuda:0')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  882.4263, -1408.3219, -1408.0712,  1585.5712,   998.8504,  1784.5726,\n",
       "         1266.0608,   737.8477], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3499, device='cuda:0')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WaitableQueue at 0x131ce69a580 maxsize=0 _queue=[{'command': 'action', 'content': {'voltage': [0, 0, 0, 0, 0, 0, 0, 0]}}, {'command': 'action', 'content': {'voltage': [0, 0, 0, 0, 0, 0, 0, 0]}}, {'command': 'action', 'content': {'voltage': [0, 0, 0, 0, 0, 0, 0, 0]}}, {'command': 'action', 'content': {'voltage': [0, 0, 0, 0, 0, 0, 0, 0]}}, {'command': 'action', 'content': {'voltage': [0, 0, 0, 0, 0, 0, 0, 0]}}, {'command': 'action', 'content': {'voltage': [0, 0, 0, 0, 0, 0, 0, 0]}}, {'command': 'action', 'content': {'voltage': [0, 0, 0, 0, 0, 0, 0, 0]}}] tasks=8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server.outQueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c9624776d510bbc019f6ff189fc1336bb72bf6a7e32ae91ed9f1b177f47b3d26"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
